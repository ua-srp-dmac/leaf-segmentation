{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da84adc-b8fa-4bef-b282-9570a810241b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grounded SAM pipeline (2025 version)\n",
    "\n",
    "This notebook implements Grounded SAM, which is a pipeline that makes bounding box predictions from input text using Grounding DINO model, and then uses Segment Anything with bounding box prompts to generate the segmentation masks.\n",
    "\n",
    "Output images with segmentation masks are saved to a specified output folder to visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8e53e-be11-4a80-b2c1-81f9fd6fc8bf",
   "metadata": {},
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7829e5e1-3895-438f-9bbd-82096a4fe7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install fiftyone==1.1.0\n",
    "!pip install pyzbar\n",
    "!pip install opencv-python\n",
    "!pip install seaborn\n",
    "!pip install openpyxl\n",
    "!pip install groundingdino-py\n",
    "!pip install segment_anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07d94f-cd39-437c-81a8-91956b8da9c2",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c04cd5a-4265-41b2-aed6-6783a5fe0e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import fiftyone as fo\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps, ImageDraw, ImageFont\n",
    "from PIL.ExifTags import TAGS\n",
    "from pyzbar.pyzbar import decode, ZBarSymbol\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_convert\n",
    "from IPython.display import display\n",
    "import argparse\n",
    "import copy\n",
    "from io import BytesIO\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data.catalog import Metadata\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "from segment_anything import build_sam, SamPredictor\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "# Set up the logger for detectron2\n",
    "setup_logger()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1200e-fc68-4c54-8738-7aef224e0dd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Grounding Dino Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07a369-dc8a-48f8-8e7c-4895e7899aa4",
   "metadata": {},
   "source": [
    "Load the Grounding DINO Model which will be used to predict bounding boxes, given a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e04f7fa8-1d6d-4130-89cf-4a1e49bc153b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7684a414173e4c2c8ccd888b29859a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GroundingDINO_SwinB.cfg.py:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e773e56647a4ddfaa14f4c30396e08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef42b1f89c9d4fc9b52efca186f15af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478fff26dd14430a82d63d6f8d0c8936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abbebf0f2f949b78b97aed923e77825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451e1bc88c8744728a82fedd3927bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96af535776c4709a2af1ab31225ed9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "groundingdino_swinb_cogcoor.pth:   0%|          | 0.00/938M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from /home/jovyan/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n",
    "\n",
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ffa60-8677-4bc7-9545-baa01093ec9f",
   "metadata": {},
   "source": [
    "### Load Segment Anything Model (SAM)\n",
    "\n",
    "Segment Anything will be used to generate segmentation leaves for detected leaf bounding boxes from Grounding DINO output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "808203b1-5caa-4ff8-b924-27d8153c3017",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-06-24 20:05:00--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.155.173.116, 18.155.173.40, 18.155.173.79, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.155.173.116|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_h_4b8939.pth.3’\n",
      "\n",
      "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   102MB/s    in 23s     \n",
      "\n",
      "2024-06-24 20:05:23 (104 MB/s) - ‘sam_vit_h_4b8939.pth.3’ saved [2564550879/2564550879]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb812bb-437c-404d-8c04-83d45dfcd2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "sam = build_sam(checkpoint=sam_checkpoint)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd33d2b-5c5d-4ae2-b5bf-a01167751ccc",
   "metadata": {},
   "source": [
    "### Initialize Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13e85bd1-58cf-4136-8d00-00cef1dcd741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iou(gtmask, test_mask):\n",
    "    intersection = np.logical_and(gtmask, test_mask)\n",
    "    union = np.logical_or(gtmask, test_mask)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return (iou_score)\n",
    "\n",
    "# Usually there is a mask for the entire plant in addition to individual leaves. \n",
    "# This function attempts to remove the full plant mask by caclculating the iou of each mask and the union of all masks.\n",
    "def check_full_plant(masks):\n",
    "    # Initialize the combined mask\n",
    "    mask_all = np.zeros(masks[0].shape, dtype=np.float32)\n",
    "\n",
    "    # Combine all masks into one\n",
    "    for mask in masks:\n",
    "        mask_all += mask.astype(np.float32)\n",
    "\n",
    "    iou_withall = []\n",
    "    # Calculate IoU for each mask with the combined mask\n",
    "    for mask in masks:\n",
    "        iou_withall.append(iou(mask, mask_all))\n",
    "\n",
    "    idx_notall = np.array(iou_withall) < 0.9\n",
    "    return idx_notall\n",
    "\n",
    "def annotate_masks_in_image(masks, image, include, random_color=True):\n",
    "    # Convert image to RGBA\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "\n",
    "    \n",
    "    # Iterate through each mask\n",
    "    for i in range(masks.shape[0]):\n",
    "        if (include[i]):\n",
    "            mask = masks[i]\n",
    "            if random_color:\n",
    "                color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "            else:\n",
    "                color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "\n",
    "            h, w = mask.shape[-2:]\n",
    "            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "            mask_image_pil = Image.fromarray((mask_image * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "            # Composite the mask with the image\n",
    "            annotated_frame_pil = Image.alpha_composite(annotated_frame_pil, mask_image_pil)\n",
    "    \n",
    "    return np.array(annotated_frame_pil)\n",
    "\n",
    "# Returns relative bbox for FiftyOne and cropped mask\n",
    "def get_bbox_for_plant(file_name):\n",
    "    \n",
    "    image_id = next(img['id'] for img in coco_data['images'] if img['file_name'] == file_name)\n",
    "\n",
    "    # Get annotations for the image ID with label 'leaf'\n",
    "    leaf_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id and coco_data['categories'][ann['category_id']]['name'] == 'leaf']\n",
    "\n",
    "    # Calculate the bounding box that encompasses all leaf annotations\n",
    "    if leaf_annotations:\n",
    "        x_min = min(ann['bbox'][0] for ann in leaf_annotations)\n",
    "        y_min = min(ann['bbox'][1] for ann in leaf_annotations)\n",
    "        x_max = max(ann['bbox'][0] + ann['bbox'][2] for ann in leaf_annotations)\n",
    "        y_max = max(ann['bbox'][1] + ann['bbox'][3] for ann in leaf_annotations)\n",
    "        plant_bbox = [x_min, y_min, x_max - x_min, y_max - y_min]\n",
    "    else:\n",
    "        plant_bbox = None\n",
    "\n",
    "    return plant_bbox\n",
    "\n",
    "def convert_bbox_to_full_image(bbox_cropped, bbox_full_image):\n",
    "    x_min_cropped, y_min_cropped, width_cropped, height_cropped = bbox_cropped\n",
    "    x_min_full, y_min_full, width_full, height_full = bbox_full_image\n",
    "\n",
    "    x_min = x_min_full + x_min_cropped\n",
    "    y_min = y_min_full + y_min_cropped\n",
    "    x_max = x_min + width_cropped\n",
    "    y_max = y_min + height_cropped\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def adjust_mask(mask_cropped, bbox_cropped, full_image_shape):\n",
    "\n",
    "    x_min_cropped, y_min_cropped, width_cropped, height_cropped = bbox_cropped\n",
    "    full_image_height, full_image_width = full_image_shape[:2]\n",
    "\n",
    "    if len(mask_cropped.shape) == 3 and mask_cropped.shape[0] == 1:\n",
    "        mask_cropped = mask_cropped[0]\n",
    "\n",
    "    # Create an empty mask for the full image\n",
    "    mask_full = np.zeros((full_image_height, full_image_width), dtype=bool)\n",
    "\n",
    "    # Determine the location in the full image where the cropped mask should be placed\n",
    "    x_min_full = int(x_min_cropped)\n",
    "    y_min_full = int(y_min_cropped)\n",
    "\n",
    "    # Place the cropped mask in the corresponding location in the full image mask\n",
    "    mask_full[y_min_full:y_min_full+mask_cropped.shape[0], x_min_full:x_min_full+mask_cropped.shape[1]] = mask_cropped\n",
    "\n",
    "    return mask_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07ad79a9-9c16-41d1-bbef-ab92fada8704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "# x_min: The x-coordinate of the top-left corner of the bounding box.\n",
    "# y_min: The y-coordinate of the top-left corner of the bounding box.\n",
    "# width: The width of the bounding box.\n",
    "# height: The height of the bounding box.\n",
    "\n",
    "def visualize_bbox(image_path, bbox):\n",
    "    img = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20875cc1-349f-4aa9-9c6d-d8de58ea5348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Returns relative bbox for FiftyOne and cropped mask\n",
    "def relative_mask_bbox(mask, file_name, image):\n",
    "    \n",
    "    plant_bbox = get_bbox_for_plant(file_name)\n",
    "    # Assume we have the bbox of the full image\n",
    "    \n",
    "    if not plant_bbox:\n",
    "        plant_bbox = [0, 0, image.shape[1], image.shape[0]]\n",
    "    \n",
    "    bbox_full_image = [0, 0, image.shape[1], image.shape[0]]  # Full image bounding box\n",
    "\n",
    "    # Convert bbox to full image coordinates\n",
    "    bbox_full = convert_bbox_to_full_image(plant_bbox, bbox_full_image)\n",
    "    mask_full = adjust_mask(mask, plant_bbox, image.shape)\n",
    "    \n",
    "\n",
    "    # Find the coordinates of the nonzero elements in the mask\n",
    "    nonzero_rows, nonzero_cols = np.nonzero(mask_full)\n",
    "\n",
    "    # Calculate the bounding box of the leaf\n",
    "    x_min = np.min(nonzero_cols)\n",
    "    x_max = np.max(nonzero_cols)\n",
    "    y_min = np.min(nonzero_rows)\n",
    "    y_max = np.max(nonzero_rows)\n",
    "\n",
    "    # Create an array representing the bounding box [x_min, y_min, width, height]\n",
    "    cropped_mask = mask_full[y_min:y_max, x_min:x_max]\\\n",
    "\n",
    "    return x_min, y_min, x_max, y_max, cropped_mask\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2fff45-dc9f-41a1-91fe-a29274b83558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_bbox_large(bbox, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Check if bbox covers a large portion of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - bbox: List representing the bounding box [x, y, width, height].\n",
    "    - threshold: Threshold percentage for considering a bbox as covering a large portion of the image.\n",
    "\n",
    "    Returns:\n",
    "    - True if bbox covers a large portion of the image, False otherwise.\n",
    "    \"\"\"\n",
    "    _, _, width, height = bbox\n",
    "    bbox_area = width * height\n",
    "    image_area = 1.0  # Assuming image area is 1 (normalized coordinates)\n",
    "    return bbox_area >= threshold * image_area\n",
    "\n",
    "def filter_large_bboxes(boxes, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Filter out bounding boxes that cover a large portion of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - boxes: Tensor of bounding boxes in the format (left, top, width, height).\n",
    "    - image_size: Tuple representing the size of the image (width, height).\n",
    "    - threshold: Threshold percentage for considering a bbox as covering a large portion of the image.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of bounding boxes that do not cover a large portion of the image.\n",
    "    \"\"\"\n",
    "    filtered_boxes = []\n",
    "    for bbox in boxes:\n",
    "        if not is_bbox_large(bbox, threshold):\n",
    "            filtered_boxes.append(bbox)\n",
    "    if len(filtered_boxes) > 0:\n",
    "        return torch.stack(filtered_boxes)\n",
    "    else:\n",
    "        # Return an empty tensor with the same shape as the input boxes\n",
    "        return torch.empty_like(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363ea9b-dee6-4536-92a7-59411e16f119",
   "metadata": {},
   "source": [
    "# Running Grounded SAM on your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2437332e-f435-4de1-a050-0676852694d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(thing_classes=['leaf', 'qr', 'red-square'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# name of output folder\n",
    "output_file_name = '2025_grounded_sam'\n",
    "\n",
    "data_path = '/home/jovyan/work/data/2024-03-14_leaves'\n",
    "\n",
    "coco_annotation_path = '/home/jovyan/work/data/2025_leaves.json'\n",
    "with open(coco_annotation_path) as f:\n",
    "    coco_data = json.load(f)\n",
    "    coco_annotations = coco_data['annotations']\n",
    "    coco_images_info = coco_data['images']\n",
    "    coco_categories = coco_data['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d74d00-4b68-47f3-843d-dbc3fe450638",
   "metadata": {},
   "source": [
    "Next, we load the images and annotations into a FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2aa46450-a34d-4cce-a954-e5b7668a9ecc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 176/176 [46.7s elapsed, 0s remaining, 2.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=f\"{data_path}\",\n",
    "    labels_path=f\"{coco_annotation_path}\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    name=f\"{today}_grounded_sam\",\n",
    "    label_types=\"segmentations\",\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "22158348-7b28-44ff-b6d9-fd39df9e4c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaf\n",
      "   0% ||----------------|   0/176 [17.5ms elapsed, ? remaining, ? samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:1044: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12% |██---------------|  22/176 [22.7s elapsed, 2.7m remaining, 0.9 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/1864294252.py:6: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  iou_score = np.sum(intersection) / np.sum(union)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 176/176 [3.6m elapsed, 0s remaining, 0.7 samples/s]    \n",
      "Predicted Classes: ['leaf']\n",
      "417\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.5s elapsed, 0s remaining, 13.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.4s elapsed, 0s remaining, 58.1 samples/s]          \n",
      "leaf . partial leaf\n",
      " 100% |█████████████████| 176/176 [3.8m elapsed, 0s remaining, 0.7 samples/s]      \n",
      "Predicted Classes: ['leaf', 'leaf leaf', 'leaf partial', 'leaf partial leaf', 'partial leaf']\n",
      "529\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.7s elapsed, 0s remaining, 12.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.7s elapsed, 0s remaining, 54.8 samples/s]      \n",
      "leaf . long narrow leaf\n",
      " 100% |█████████████████| 176/176 [3.7m elapsed, 0s remaining, 0.7 samples/s]      \n",
      "Predicted Classes: ['leaf', 'leaf leaf', 'leaf long narrow leaf', 'leaf narrow leaf', 'long narrow leaf', 'narrow leaf']\n",
      "455\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.1s elapsed, 0s remaining, 12.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.5s elapsed, 0s remaining, 56.0 samples/s]      \n",
      "leaf . long narrow leaf . curled leaf\n",
      " 100% |█████████████████| 176/176 [3.9m elapsed, 0s remaining, 0.7 samples/s]      \n",
      "Predicted Classes: ['curled leaf', 'leaf', 'leaf curled', 'leaf curled leaf', 'leaf leaf', 'leaf leaf curled', 'leaf leaf curled leaf', 'leaf long narrow leaf', 'leaf long narrow leaf curled leaf', 'leaf narrow', 'leaf narrow leaf', 'leaf narrow leaf curled', 'leaf narrow leaf curled leaf', 'long narrow leaf', 'long narrow leaf curled', 'narrow leaf', 'narrow leaf curled leaf']\n",
      "604\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [8.1s elapsed, 0s remaining, 10.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.8s elapsed, 0s remaining, 51.9 samples/s]      \n",
      "leaf . partial leaf . curled leaf . red leaf . long narrow leaf\n",
      " 100% |█████████████████| 176/176 [3.9m elapsed, 0s remaining, 0.6 samples/s]    \n",
      "Predicted Classes: ['curled leaf', 'curled leaf long narrow leaf', 'curled leaf narrow leaf', 'curled long narrow leaf', 'curled narrow leaf', 'curled red leaf', 'leaf', 'leaf curled', 'leaf curled leaf', 'leaf curled leaf leaf', 'leaf curled leaf long narrow leaf', 'leaf curled leaf narrow leaf', 'leaf curled long narrow leaf', 'leaf curled narrow', 'leaf curled narrow leaf', 'leaf leaf curled leaf', 'leaf long narrow leaf', 'leaf narrow', 'leaf narrow leaf', 'leaf partial curled leaf', 'leaf partial curled leaf long narrow leaf', 'leaf partial curled leaf narrow leaf', 'leaf partial curled narrow leaf', 'leaf partial leaf curled', 'leaf partial leaf curled leaf', 'leaf partial leaf curled leaf leaf', 'leaf partial leaf curled leaf long narrow leaf', 'leaf partial leaf curled leaf narrow', 'leaf partial leaf curled leaf narrow leaf', 'leaf partial leaf curled narrow leaf', 'long narrow leaf', 'narrow leaf', 'partial curled narrow leaf', 'partial leaf', 'partial leaf curled', 'partial leaf curled leaf', 'partial leaf curled leaf narrow', 'partial leaf curled leaf narrow leaf', 'partial leaf curled leaf red leaf', 'partial leaf red leaf', 'partial narrow leaf', 'red leaf', 'red leaf narrow leaf']\n",
      "558\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.6s elapsed, 0s remaining, 12.2 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.7s elapsed, 0s remaining, 54.8 samples/s]      \n",
      "leaf . green leaf with crystals . long narrow leaf . sprouting leaf\n",
      " 100% |█████████████████| 176/176 [3.7m elapsed, 0s remaining, 0.8 samples/s]      \n",
      "Predicted Classes: ['crystals', 'green leaf', 'green leaf leaf', 'green leaf leaf leaf', 'green leaf leafrouting leaf', 'green leaf narrow leaf', 'green leaf narrow leaf leaf', 'green leaf narrow leaf sprouting leaf', 'green leaf narrow leafrouting leaf', 'green leaf narrow leafting leaf', 'green leaf narrow sprouting leaf', 'green leaf sprouting leaf', 'green leafrouting leaf', 'green leafting', 'green leafting leaf', 'leaf', 'leaf green leaf', 'leaf green leaf leaf', 'leaf green leaf narrow leaf', 'leaf green leaf narrow leaf leaf', 'leaf green leaf narrow leaf sprouting leaf', 'leaf green leaf narrow leafting leaf', 'leaf green leaf narrowrouting leaf', 'leaf green leaf sprouting leaf', 'leaf green leafting leaf', 'leaf narrow leaf', 'leaf narrow leaf leaf', 'leafrouting leaf', 'long narrow leaf', 'long narrow leaf sprouting leaf', 'long narrow leafting', 'long narrow leafting leaf', 'narrow leaf', 'narrow leaf leaf', 'narrow leaf sprouting leaf', 'narrow leafrouting leaf', 'narrow leafting', 'narrow leafting leaf', 'sprouting leaf']\n",
      "462\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [6.7s elapsed, 0s remaining, 14.1 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.5s elapsed, 0s remaining, 58.1 samples/s]      \n",
      "leaf . green leaf with crystals . long narrow leaf . sprouting leaf . broad leaf\n",
      " 100% |█████████████████| 176/176 [3.7m elapsed, 0s remaining, 0.8 samples/s]      \n",
      "Predicted Classes: ['##routing leaf', 'green leaf', 'green leaf broad leaf', 'green leaf leaf', 'green leaf leaf broad leaf', 'green leaf leafting broad leaf', 'green leaf leafting leaf broad leaf', 'green leaf narrow', 'green leaf narrow leaf', 'green leaf narrow leaf broad', 'green leaf narrow leaf broad leaf', 'green leaf narrow leaf leaf', 'green leaf narrow leaf leaf broad leaf', 'green leaf narrow leaf sprouting leaf broad', 'green leaf narrow leafrouting leaf', 'green leaf narrow leafrouting leaf broad', 'green leaf narrow leafrouting leaf broad leaf', 'green leaf narrow leafting', 'green leaf narrow leafting leaf', 'green leaf narrow leafting leaf broad leaf', 'green leaf narrow sprouting leaf', 'green leaf narrowrouting leaf broad leaf', 'green leaf narrowting', 'green leaf narrowting leaf', 'green leaf sprouting leaf', 'green leafrouting leaf', 'green leafrouting leaf broad', 'green leafting', 'green leafting leaf', 'green leafting leaf broad', 'green leafting leaf broad leaf', 'green narrow leaf', 'leaf green leaf', 'leaf green leaf narrow leaf', 'leaf green leafting', 'leaf green leafting leaf', 'leaf narrow leaf broad leaf', 'long narrow leaf', 'long narrow leaf broad leaf', 'long narrow leaf sprouting leaf', 'narrow leaf', 'narrow leaf broad', 'narrow leaf broad leaf', 'narrow leaf sprouting leaf', 'narrow leaf sprouting leaf broad leaf', 'narrow leafrouting leaf', 'narrow leafrouting leaf broad leaf', 'narrow leafting', 'narrow leafting broad', 'narrow leafting broad leaf', 'narrow leafting leaf', 'narrow leafting leaf broad leaf', 'sprouting leaf']\n",
      "445\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [6.6s elapsed, 0s remaining, 14.6 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.4s elapsed, 0s remaining, 60.7 samples/s]      \n",
      "leaf . green leaf with white texture . long narrow leaf . sprouting leaf . red leaf\n",
      " 100% |█████████████████| 176/176 [3.9m elapsed, 0s remaining, 0.7 samples/s]    \n",
      "Predicted Classes: ['##routing leaf', 'green leaf', 'green leaf leaf', 'green leaf narrow', 'green leaf narrow leaf', 'green leaf narrow leaf leaf', 'green leaf narrow leaf sprouting leaf', 'green leaf narrow leafrouting leaf', 'green leaf narrow leafting leaf', 'green leaf narrow sprouting leaf', 'green leaf narrowting leaf', 'green leaf sprouting leaf', 'green leafrouting leaf', 'green leafting', 'green leafting leaf', 'green narrow leaf sprouting leaf', 'green narrow leafting leaf red leaf', 'leaf green leaf', 'leaf green leaf leaf', 'leaf green leaf leafting leaf', 'leaf green leaf narrow leaf', 'leaf green leaf narrow leaf sprouting leaf', 'leaf green leaf narrow leafrouting leaf', 'leaf green leafrouting leaf', 'leaf green leafting', 'leaf green leafting leaf', 'leaf narrow leaf sprouting leaf', 'leaf narrow leafting leaf', 'leafrouting leaf', 'long narrow leaf', 'long narrow leaf sprouting leaf', 'long narrow leafting', 'narrow leaf', 'narrow leaf leaf', 'narrow leaf red leaf', 'narrow leaf sprouting leaf', 'narrow leafrouting leaf', 'narrow leafting', 'narrow leafting leaf', 'narrowrouting leaf', 'red leaf', 'sprouting leaf', 'sprouting leaf red leaf']\n",
      "542\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.1s elapsed, 0s remaining, 13.3 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.6s elapsed, 0s remaining, 55.8 samples/s]      \n",
      "leaf . green leaf with white speckles . long narrow leaf . sprouting leaf .  broad leaf . partial leaf . leaf with red tip . curled leaf\n",
      " 100% |█████████████████| 176/176 [3.8m elapsed, 0s remaining, 0.8 samples/s]    \n",
      "Predicted Classes: ['##ting leaf', '##ting partial leaf curled', 'broad leaf partial leaf curled leaf', 'broad partial leaf curled leaf', 'green leaf', 'green leaf broad leaf', 'green leaf broad leaf curled', 'green leaf broad leaf curled leaf', 'green leaf broad leaf leaf', 'green leaf curled', 'green leaf curled leaf', 'green leaf leaf', 'green leaf leaf broad leaf partial leaf curled leaf', 'green leaf leaf curled', 'green leaf leaf leaf', 'green leaf narrow leaf', 'green leaf narrow leaf broad leaf', 'green leaf narrow leaf broad leaf curled leaf', 'green leaf narrow leaf broad leaf partial leaf', 'green leaf narrow leaf curled leaf', 'green leaf narrow leafrouting leaf broad leaf curled leaf', 'green leaf narrow leafting broad leaf curled leaf', 'green leaf partial curled', 'green leaf partial leaf', 'green leaf partial leaf curled', 'green leaf partial leaf curled leaf', 'green leaf partial leaf leaf', 'green leaf sprouting leaf curled leaf', 'green leafrouting leaf', 'green leafrouting leaf curled leaf', 'green leafrouting leaf leaf curled leaf', 'green leafting', 'green leafting curled', 'green leafting curled leaf', 'green leafting leaf', 'green leafting leaf curled leaf', 'green leafting leaf leaf', 'green leafting leaf partial leaf curled leaf', 'green leafting leaf partial leaf leaf curled leaf', 'green partial leaf curled leaf', 'leaf', 'leaf broad leaf leaf', 'leaf broad leaf partial leaf curled leaf', 'leaf green leaf', 'leaf green leaf broad', 'leaf green leaf curled leaf', 'leaf green leaf leaf', 'leaf partial leaf curled leaf', 'long narrow leaf', 'narrow leaf', 'narrow leaf broad leaf', 'narrow leaf broad leaf curled leaf', 'narrow leaf broad leaf leaf', 'narrow leaf broad leaf partial leaf curled leaf', 'narrow leaf curled', 'narrow leaf leaf', 'narrow leaf leaf partial leaf curled leaf', 'narrow leaf partial', 'narrow leaf partial curled', 'narrow leaf partial leaf', 'narrow leaf partial leaf curled', 'narrow leaf partial leaf curled leaf', 'narrow leafrouting leaf broad leaf curled leaf', 'narrow leafting', 'narrow leafting broad leaf', 'narrow leafting leaf', 'narrow leafting leaf curled', 'narrow leafting leaf leaf', 'narrow leafting leaf partial leaf curled', 'narrow leafting partial leaf curled', 'partial leaf', 'partial leaf curled leaf', 'tip']\n",
      "501\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [6.6s elapsed, 0s remaining, 17.7 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.5s elapsed, 0s remaining, 59.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from fiftyone import ViewField as F_fo\n",
    "import csv\n",
    "\n",
    "sam_image_dir = '/home/jovyan/work/data/leaves_cropped'\n",
    "sam_output_folder = '/home/jovyan/work/data/grounded_sam/2025-02-06_prompt1'\n",
    "save_output_img = False\n",
    "\n",
    "TEXT_PROMPTS = [\n",
    "    \"leaf\",\n",
    "    \"leaf . partial leaf\",\n",
    "    \"leaf . long narrow leaf\",\n",
    "    \"leaf . long narrow leaf . curled leaf\",\n",
    "    \"leaf . partial leaf . curled leaf . red leaf . long narrow leaf\",\n",
    "    \"leaf . green leaf with crystals . long narrow leaf . sprouting leaf\",\n",
    "    \"leaf . green leaf with crystals . long narrow leaf . sprouting leaf . broad leaf\",\n",
    "    \"leaf . green leaf with white texture . long narrow leaf . sprouting leaf . red leaf\",\n",
    "    \"leaf . green leaf with white speckles . long narrow leaf . sprouting leaf .  broad leaf . partial leaf . leaf with red tip . curled leaf\",\n",
    "]\n",
    "\n",
    "BOX_TRESHOLD = 0.3\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "# Path to save metrics\n",
    "metrics_csv_path = '/home/jovyan/work/data/grounded_sam/metrics_results.csv'\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "file_exists = os.path.isfile(metrics_csv_path)\n",
    "\n",
    "# Initialize the CSV file with headers if it doesn't exist\n",
    "if not file_exists:\n",
    "    with open(metrics_csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Prompt', 'mAP', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "for TEXT_PROMPT in TEXT_PROMPTS:\n",
    "    \n",
    "    print(TEXT_PROMPT)\n",
    "\n",
    "    with fo.ProgressBar() as pb:\n",
    "        for sample in pb(dataset):\n",
    "\n",
    "            # Load full image\n",
    "            image = Image.open(sample.filepath)\n",
    "            image = ImageOps.exif_transpose(image)  # Ensure correct orientation\n",
    "            image_arr = np.array(image)\n",
    "            h, w, _ = image_arr.shape\n",
    "\n",
    "            # --------------------- Grounding Dino --------------------\n",
    "            file_name = os.path.basename(sample.filepath)\n",
    "            name, ext = os.path.splitext(file_name)\n",
    "\n",
    "\n",
    "            # get cropped images\n",
    "            file_path = f\"{sam_image_dir}/{name}_cropped{ext}\"\n",
    "\n",
    "            # if cropped image exists, use it\n",
    "            if os.path.exists(file_path):\n",
    "                image_source, dino_image = load_image(file_path)\n",
    "            # otherwise, use full image\n",
    "            else:\n",
    "                image_source, dino_image = load_image(sample.filepath)\n",
    "                # update to use full image\n",
    "                file_path = sample.filepath\n",
    "\n",
    "            # Get prediction bboxes from Grounding DINO\n",
    "            boxes, logits, phrases = predict(\n",
    "                model=groundingdino_model,\n",
    "                image=dino_image,\n",
    "                caption=TEXT_PROMPT,\n",
    "                box_threshold=BOX_TRESHOLD,\n",
    "                text_threshold=TEXT_TRESHOLD,\n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            boxes = filter_large_bboxes(boxes, threshold=0.9)\n",
    "\n",
    "            if os.path.exists(file_path) and save_output_img:\n",
    "                # Annotate image with bounding boxes\n",
    "                annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "                annotated_frame = annotated_frame[..., ::-1]  # Convert BGR to RGB\n",
    "\n",
    "                # Save Grouding DINO bbox visualization to image\n",
    "                dino_result = Image.fromarray(annotated_frame)\n",
    "                dino_result.save(os.path.join(sam_output_folder, f\"{name}_dino_bboxes.png\"))\n",
    "\n",
    "            # Set image for SAM\n",
    "            sam_predictor.set_image(image_source)\n",
    "\n",
    "            # Convert box format and apply transformations\n",
    "            H, W, _ = image_source.shape\n",
    "            boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "            transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2]).to(DEVICE)\n",
    "\n",
    "            # Initialize list of FiftyOne detections for grounded sam predictions\n",
    "            grounded_sam_detections = []\n",
    "\n",
    "            if transformed_boxes.size(0) > 0:\n",
    "                masks, _, _ = sam_predictor.predict_torch(\n",
    "                    point_coords=None,\n",
    "                    point_labels=None,\n",
    "                    boxes=transformed_boxes,\n",
    "                    multimask_output=False,\n",
    "                )\n",
    "\n",
    "                masks_cpu = masks.cpu().numpy()\n",
    "                idx_not_full_plant = check_full_plant(masks_cpu)\n",
    "\n",
    "                if save_output_img:\n",
    "                    original_image = Image.open(file_path)\n",
    "                    image_array = np.array(original_image)\n",
    "                    annotated_frame_with_mask = annotate_masks_in_image(\n",
    "                        masks_cpu,\n",
    "                        image_array,\n",
    "                        idx_not_full_plant\n",
    "                    )\n",
    "                    output_image = Image.fromarray(annotated_frame_with_mask)\n",
    "                    output_image.save(f\"{sam_output_folder}/{name}_grounded_sam_masks.png\")\n",
    "\n",
    "                # Add Grounded SAM detections to FiftyOne dataset\n",
    "                for i in range(masks.shape[0]):\n",
    "                    if idx_not_full_plant[i]:\n",
    "                        mask = masks[i].cpu().numpy()\n",
    "\n",
    "                        x1, y1, x2, y2, bbox_mask = relative_mask_bbox(mask, file_name, image_arr)\n",
    "\n",
    "                        # Get confidence store\n",
    "                        confidence =  float(logits[i])\n",
    "                        label = phrases[i]\n",
    "\n",
    "                        grounded_sam_detections.append(fo.Detection(\n",
    "                            label=label,\n",
    "                            bounding_box=[\n",
    "                                min(x1, x2) / image.width,\n",
    "                                min(y1, y2) / image.height,\n",
    "                                abs(x2 - x1) / image.width,\n",
    "                                abs(y2 - y1) / image.height\n",
    "                            ],\n",
    "                            mask=bbox_mask,\n",
    "                            confidence=confidence\n",
    "                        ))\n",
    "\n",
    "            # Update sample with Grounded SAM detections\n",
    "            sample[\"grounded_sam_predictions\"] = fo.Detections(detections=grounded_sam_detections)\n",
    "            sample[\"height\"] = h\n",
    "            sample[\"width\"] = w\n",
    "            sample.tags.append(os.path.basename(sample.filepath))\n",
    "            sample.save()\n",
    "        \n",
    "    # List unique classes in the predicted labels\n",
    "    unique_pred_classes = dataset.distinct(\"grounded_sam_predictions.detections.label\")\n",
    "    print(\"Predicted Classes:\", unique_pred_classes)\n",
    "    \n",
    "    # Replace all leaf-related detection's labels with \"leaf\"\n",
    "    mapping = {k: \"leaf\" for k in unique_pred_classes}\n",
    "    normalized_view = dataset.map_labels(\"grounded_sam_predictions\", mapping)\n",
    "\n",
    "    counts = normalized_view.count_values(\"grounded_sam_predictions.detections.label\")\n",
    "    print(counts[\"leaf\"])\n",
    "\n",
    "    # Filter ground truth to include only 'leaf' detections\n",
    "    leaf_view = normalized_view.filter_labels(\"ground_truth\", F_fo(\"label\") == \"leaf\")\n",
    "\n",
    "    # Evaluate detections treating all as 'leaf'\n",
    "    results = leaf_view.evaluate_detections(\n",
    "        \"grounded_sam_predictions\",  # Normalized predictions field in the cloned dataset\n",
    "        gt_field=\"ground_truth\",     # Ground truth field\n",
    "        compute_mAP=True,\n",
    "        eval_key=\"eval\"\n",
    "    )\n",
    "\n",
    "    # Get mAP for 'leaf' class\n",
    "    map_score = results.mAP()\n",
    "    tp = leaf_view.sum(\"eval_tp\")\n",
    "    fp = leaf_view.sum(\"eval_fp\")\n",
    "    fn = leaf_view.sum(\"eval_fn\")\n",
    "\n",
    "    # Calculate Precision, Recall, and F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    # Append metrics to CSV\n",
    "    with open(metrics_csv_path, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([TEXT_PROMPT, map_score, tp, fp, fn, precision, recall, f1_score])\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b18363-221b-450a-83ed-5a80e0819154",
   "metadata": {},
   "source": [
    "## Calculate Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0cb964c9-f585-40c8-ba01-caba14079a89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [6.4s elapsed, 0s remaining, 17.8 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.3s elapsed, 0s remaining, 63.9 samples/s]      \n",
      "Mean Average Precision (mAP) for 'leaf': 0.0081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        leaf       0.37      0.02      0.04      1001\n",
      "\n",
      "   micro avg       0.37      0.02      0.04      1001\n",
      "   macro avg       0.37      0.02      0.04      1001\n",
      "weighted avg       0.37      0.02      0.04      1001\n",
      "\n",
      "TP (leaf): 22\n",
      "FP (leaf): 479\n",
      "FN (leaf): 979\n",
      "{'accuracy': 0.014864864864864866, 'precision': 0.043912175648702596, 'recall': 0.02197802197802198, 'fscore': 0.029294274300932094, 'support': 1001}\n",
      "{'green leafting leaf curled leaf': 1, 'green leafrouting leaf leaf curled leaf': 1, 'green leaf narrow leafrouting leaf broad leaf curled leaf': 1, 'green leaf broad leaf curled leaf': 3, 'green leafting': 1, 'narrow leaf': 90, '##ting leaf': 1, 'leaf broad leaf leaf': 1, 'narrow leaf broad leaf': 9, 'green leaf sprouting leaf curled leaf': 1, 'leaf green leaf curled leaf': 1, 'leaf green leaf leaf': 1, 'green leaf narrow leaf broad leaf partial leaf': 1, 'green leaf narrow leafting broad leaf curled leaf': 1, 'green leaf leaf leaf': 1, 'green leaf': 169, 'leaf': 60, 'green partial leaf curled leaf': 2, 'green leafting curled': 2, 'narrow leaf partial leaf curled': 3, 'green leaf leaf curled': 1, 'partial leaf curled leaf': 13, 'broad leaf partial leaf curled leaf': 2, 'tip': 2, 'long narrow leaf': 19, 'leaf green leaf broad': 1, 'narrow leafting partial leaf curled': 1, 'broad partial leaf curled leaf': 1, 'leaf green leaf': 1, 'green leaf partial leaf curled leaf': 5, 'narrow leaf broad leaf leaf': 4, 'green leaf broad leaf curled': 1, 'green leaf narrow leaf': 2, 'narrow leaf leaf partial leaf curled leaf': 1, 'narrow leaf broad leaf curled leaf': 2, 'narrow leaf curled': 2, 'narrow leaf partial leaf': 5, 'green leaf partial leaf': 2, 'green leafting leaf': 7, 'green leafting leaf partial leaf leaf curled leaf': 1, 'green leaf narrow leaf broad leaf curled leaf': 2, 'narrow leafting leaf curled': 1, 'narrow leaf partial': 1, 'green leaf narrow leaf curled leaf': 1, 'green leafrouting leaf': 1, 'narrow leaf leaf': 4, 'narrow leaf broad leaf partial leaf curled leaf': 3, 'narrow leafting': 1, 'narrow leafting leaf leaf': 1, 'green leaf partial curled': 1, 'narrow leafting leaf': 1, 'green leaf leaf broad leaf partial leaf curled leaf': 1, 'narrow leafting broad leaf': 1, '##ting partial leaf curled': 2, 'green leafrouting leaf curled leaf': 2, 'green leaf partial leaf leaf': 1, 'narrow leaf partial curled': 1, 'green leaf curled leaf': 10, 'narrow leafting leaf partial leaf curled': 1, 'narrow leafrouting leaf broad leaf curled leaf': 1, 'green leafting leaf leaf': 3, 'green leaf broad leaf leaf': 1, 'green leafting leaf partial leaf curled leaf': 1, 'green leaf broad leaf': 4, 'leaf partial leaf curled leaf': 1, 'green leaf narrow leaf broad leaf': 3, 'narrow leaf partial leaf curled leaf': 4, 'partial leaf': 7, 'green leaf partial leaf curled': 1, 'green leaf leaf': 12, 'green leafting curled leaf': 2, 'leaf broad leaf partial leaf curled leaf': 1, 'green leaf curled': 1}\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Filter ground truth and predictions to only include 'leaf' detections\n",
    "leaf_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"leaf\")\n",
    "\n",
    "# Evaluate detections only for 'leaf' class\n",
    "results = leaf_view.evaluate_detections(\n",
    "    \"grounded_sam_predictions\",  # Predicted detections field\n",
    "    gt_field=\"ground_truth\",     # Ground truth field\n",
    "    compute_mAP=True,\n",
    "    eval_key=\"eval\"\n",
    ")\n",
    "\n",
    "# Get mAP for 'leaf' class\n",
    "map_score = results.mAP()\n",
    "print(f\"Mean Average Precision (mAP) for 'leaf': {map_score:.4f}\")\n",
    "\n",
    "# Print classification report for 'leaf'\n",
    "results.print_report(classes=[\"leaf\"])\n",
    "\n",
    "# Print TP, FP, FN specifically for 'leaf'\n",
    "print(\"TP (leaf): %d\" % leaf_view.sum(\"eval_tp\"))\n",
    "print(\"FP (leaf): %d\" % leaf_view.sum(\"eval_fp\"))\n",
    "print(\"FN (leaf): %d\" % leaf_view.sum(\"eval_fn\"))\n",
    "\n",
    "print(results.metrics())\n",
    "\n",
    "counts = dataset.count_values(\"grounded_sam_predictions.detections.label\")\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e14b5c44-5d6f-40af-a7f0-53bfc05e9dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417\n",
      "Evaluating detections...\n",
      " 100% |█████████████████| 169/169 [7.0s elapsed, 0s remaining, 14.4 samples/s]      \n",
      "Performing IoU sweep...\n",
      " 100% |█████████████████| 169/169 [2.4s elapsed, 0s remaining, 55.7 samples/s]          \n",
      "Mean Average Precision (mAP) for 'leaf': 0.3003\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        leaf       0.96      0.40      0.56      1001\n",
      "\n",
      "   micro avg       0.96      0.40      0.56      1001\n",
      "   macro avg       0.96      0.40      0.56      1001\n",
      "weighted avg       0.96      0.40      0.56      1001\n",
      "\n",
      "TP (leaf): 399\n",
      "FP (leaf): 18\n",
      "FN (leaf): 602\n",
      "{'accuracy': 0.3915603532875368, 'precision': 0.9568345323741008, 'recall': 0.3986013986013986, 'fscore': 0.5627644569816643, 'support': 1001}\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Replace all leaf-related detection's labels with \"leaf\"\n",
    "mapping = {k: \"leaf\" for k in unique_pred_classes}\n",
    "normalized_view = dataset.map_labels(\"grounded_sam_predictions\", mapping)\n",
    "\n",
    "counts = normalized_view.count_values(\"grounded_sam_predictions.detections.label\")\n",
    "print(counts[\"leaf\"])\n",
    "\n",
    "# Filter ground truth to include only 'leaf' detections\n",
    "leaf_view = normalized_view.filter_labels(\"ground_truth\", F(\"label\") == \"leaf\")\n",
    "\n",
    "# Evaluate detections treating all as 'leaf'\n",
    "results = leaf_view.evaluate_detections(\n",
    "    \"grounded_sam_predictions\",  # Normalized predictions field in the cloned dataset\n",
    "    gt_field=\"ground_truth\",     # Ground truth field\n",
    "    compute_mAP=True,\n",
    "    eval_key=\"eval\"\n",
    ")\n",
    "\n",
    "# Get mAP for 'leaf' class\n",
    "map_score = results.mAP()\n",
    "print(f\"Mean Average Precision (mAP) for 'leaf': {map_score:.4f}\")\n",
    "\n",
    "# Print classification report for 'leaf'\n",
    "results.print_report(classes=[\"leaf\"])\n",
    "\n",
    "# Print TP, FP, FN specifically for 'leaf'\n",
    "print(\"TP (leaf): %d\" % leaf_view.sum(\"eval_tp\"))\n",
    "print(\"FP (leaf): %d\" % leaf_view.sum(\"eval_fp\"))\n",
    "print(\"FN (leaf): %d\" % leaf_view.sum(\"eval_fn\"))\n",
    "\n",
    "# Print complete metrics\n",
    "print(results.metrics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "54f9d849-8477-4dda-9c06-fe0ebe34abc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FiftyOne stores its \n",
    "def convert_to_pixels(bbox, height, width):\n",
    "    x1, y1, w, h = bbox\n",
    "    return [x1 * width, y1 * height, (x1 + w) * width, (y1 + h) * height]\n",
    "\n",
    "def calculate_mask(detections, height, width):\n",
    "\n",
    "    # rebuild full mask\n",
    "    mask = np.zeros((height, width), dtype=bool)\n",
    "    for d in detections:\n",
    "        if d.label == 'leaf':\n",
    "            bbox = d.bounding_box\n",
    "            x1, y1, x2, y2 = convert_to_pixels(bbox, height, width)\n",
    "            mask[round(y1):round(y2), round(x1):round(x2)] |= d.mask\n",
    "    return mask\n",
    "\n",
    "def calculate_iou_and_dice(gt_mask, pred_mask):\n",
    "    \n",
    "    intersect = np.logical_and(gt_mask, pred_mask)\n",
    "    union = np.logical_or(gt_mask, pred_mask)\n",
    "    intersect_pixels = np.count_nonzero(intersect)\n",
    "    union_pixels = np.count_nonzero(union)\n",
    "    \n",
    "    # Normal IoU and Dice calculation when there are objects\n",
    "    iou = intersect_pixels / union_pixels if union_pixels else None\n",
    "    dice = (2 * intersect_pixels) / (np.count_nonzero(gt_mask) + np.count_nonzero(pred_mask)) if np.count_nonzero(gt_mask) + np.count_nonzero(pred_mask) else None\n",
    "    \n",
    "    return iou, dice\n",
    "\n",
    "def visualize_masks(gt_mask, pred_mask):\n",
    "    f, axarr = plt.subplots(1, 2)\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    axarr[0].title.set_text('Ground Truth')\n",
    "    axarr[1].title.set_text('Prediction')\n",
    "    axarr[0].imshow(gt_mask)\n",
    "    axarr[1].imshow(pred_mask)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def count_leaves(detections):\n",
    "    return sum(1 for d in detections if hasattr(d, 'label') and d.label == 'leaf')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6618474-d3ad-4943-9b82-695780378b1b",
   "metadata": {},
   "source": [
    "#### Visualize single sample and test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4274c4e3-3650-4c19-a0ea-11152452670b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_6056.JPG\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = dataset.last()\n",
    "print(sample.filename)\n",
    "height, width = sample.height, sample.width\n",
    "ground_truth_mask = calculate_mask(sample.ground_truth.detections, height, width)\n",
    "prediction_mask = calculate_mask(sample.grounded_sam_predictions.detections, height, width)\n",
    "visualize_masks(ground_truth_mask, prediction_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292b328-cbea-4fc5-bf5b-49811ced8d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40d0af98-cc80-4d93-b293-e315f3fe6e46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 176/176 [9.9s elapsed, 0s remaining, 17.1 samples/s]      \n",
      "Min IOU:  0.0\n",
      "Max IOU:  0.9639692621343997\n",
      "Average IOU:  0.5092665798679746\n",
      "\n",
      "\n",
      "Min Dice:  0.0\n",
      "Max Dice:  0.9816541233306051\n",
      "Average Dice:  0.5740785251656847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# initialize dataframe from manual area file, and add column for ml_area\n",
    "df = pd.read_excel('/home/jovyan/work/data/manual_area_count.xlsx')\n",
    "\n",
    "columns_to_initialize = ['grounded_sam_area', 'ml_leaf_count', 'ground_truth_pixels', 'prediction_pixels', 'iou', 'dice', 'count_precision', 'count_recall', 'num_matches', 'tp', 'fp', 'fn',  'mask_precision', 'mask_recall']\n",
    "\n",
    "# Initialize columns to None and then convert to float\n",
    "for column in columns_to_initialize:\n",
    "    df[column] = None\n",
    "    df[column] = df[column].astype(float)\n",
    "    \n",
    "missing_from_file = []\n",
    "\n",
    "with fo.ProgressBar() as pb:\n",
    "    \n",
    "    iou_list = []\n",
    "    dice_list = []\n",
    "    \n",
    "    for sample in pb(dataset):\n",
    "\n",
    "        filename = sample.filepath.split('/')[-1].split('.')[0]\n",
    "        ground_truth_detections = sample.ground_truth.detections\n",
    "        prediction_detections = sample.grounded_sam_predictions.detections\n",
    "\n",
    "        ground_truth_mask = calculate_mask(ground_truth_detections, height, width)\n",
    "        prediction_mask = calculate_mask(prediction_detections, height, width)\n",
    "        leaf_pixels = np.count_nonzero(prediction_mask)\n",
    "\n",
    "        height, width = sample.height, sample.width\n",
    "        iou, dice = calculate_iou_and_dice(ground_truth_mask, prediction_mask)\n",
    "\n",
    "        row = df['picture'].str.startswith(filename)\n",
    "\n",
    "        df.loc[row, 'ground_truth_pixels'] = np.count_nonzero(ground_truth_mask)\n",
    "        df.loc[row, 'prediction_pixels'] = np.count_nonzero(prediction_mask)\n",
    "        df.loc[row, 'iou'] = round(iou, 3) if iou is not None else None\n",
    "        df.loc[row, 'dice'] = round(dice, 3) if dice is not None else None\n",
    "\n",
    "        iou_list.append(iou)\n",
    "        dice_list.append(dice)\n",
    "\n",
    "\n",
    "    iou_list = np.array(iou_list, dtype=float)\n",
    "\n",
    "\n",
    "    print('Min IOU: ', np.nanmin(iou_list))\n",
    "    print('Max IOU: ', np.nanmax(iou_list))\n",
    "    print('Average IOU: ', np.nanmean(iou_list))\n",
    "\n",
    "    print('\\n')\n",
    "    dice_list = np.array(dice_list, dtype=float)\n",
    "    print('Min Dice: ', np.nanmin(dice_list))\n",
    "    print('Max Dice: ', np.nanmax(dice_list))\n",
    "    print('Average Dice: ', np.nanmean(dice_list))\n",
    "\n",
    "\n",
    "# df_area = df[df['ml_area'] >= 0.0]\n",
    "# pd.set_option('display.max_rows', None)  # None means unlimited\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# # Calculate means for specific columns\n",
    "# mean_values = {\n",
    "#     'ground_truth_pixels': None,\n",
    "#     'prediction_pixels': None,\n",
    "#     'iou': np.nanmean(df['iou']),\n",
    "#     'dice': np.nanmean(df['dice']),\n",
    "#     'count_precision': df['count_precision'].mean(),\n",
    "#     'count_recall': df['count_recall'].mean(),\n",
    "#     'mask_precision': df['mask_precision'].mean(),\n",
    "#     'mask_recall': df['mask_recall'].mean(),\n",
    "#     'num_matches': None,\n",
    "# }\n",
    "\n",
    "# # Create a new DataFrame with the mean values\n",
    "# mean_df = pd.DataFrame(mean_values, index=['average'])\n",
    "\n",
    "# # Append the new DataFrame with averages to the original DataFrame\n",
    "# df = pd.concat([df, mean_df])\n",
    "\n",
    "\n",
    "# # Specify the path and name of the CSV file you want to create\n",
    "# csv_file_path = f'{sam_output_folder}/{output_folder_name}.csv'\n",
    "\n",
    "# # Export the DataFrame to CSV\n",
    "# df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0dc960-1adc-4861-a1ea-e4d23c3d59ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at 0.0.0.0.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=fcf36360-2412-4ee6-b758-7fdfa4042ee3\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa8c597fb80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(datasets['fold_0'], port=5151, address=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "13536959-8aa0-4ecd-82be-aac8b7155dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.refresh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
