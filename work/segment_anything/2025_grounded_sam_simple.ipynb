{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grounded Segment Anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements Grounded SAM, which is a pipeline that makes bounding box predictions from input text using Grounding DINO model, and then uses Segment Anything with bounding box prompts to generate the segmentation masks.\n",
    "\n",
    "Output images with segmentation masks are saved to a specified output folder to visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install segment_anything\n",
    "!pip install groundingdino-py\n",
    "!pip install pycocotools pillow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# If you have multiple GPUs, you can set the GPU to use here.\n",
    "# The default is to use the first GPU, which is usually GPU 0.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounding DINO\n",
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.inference import annotate, load_image, predict\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# segment anything\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from pycocotools.coco import COCO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grounding DINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):\n",
    "    cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)\n",
    "\n",
    "    args = SLConfig.fromfile(cache_config_file) \n",
    "    model = build_model(args)\n",
    "    args.device = device\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use this command for evaluate the Grounding DINO model\n",
    "# Or you can download the model by yourself\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "Model loaded from /home/jovyan/.cache/huggingface/hub/models--ShilongLiu--GroundingDINO/snapshots/a94c9b567a2a374598f05c584e96798a170c56fb/groundingdino_swinb_cogcoor.pth \n",
      " => _IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\n"
     ]
    }
   ],
   "source": [
    "groundingdino_model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SAM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command downloads the Segment Anything model.  If you already have the file, skip this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-06 17:19:02--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.74.12, 13.227.74.9, 13.227.74.118, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.74.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
      "Saving to: ‘sam_vit_h_4b8939.pth.1’\n",
      "\n",
      "sam_vit_h_4b8939.pt  16%[==>                 ] 399.92M  88.1MB/s    eta 26s    ^C\n"
     ]
    }
   ],
   "source": [
    "! wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sam_checkpoint = 'sam_vit_h_4b8939.pth'\n",
    "sam = build_sam(checkpoint=sam_checkpoint)\n",
    "sam.to(device=DEVICE)\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def iou(gtmask, test_mask):\n",
    "    intersection = np.logical_and(gtmask, test_mask)\n",
    "    union = np.logical_or(gtmask, test_mask)\n",
    "    iou_score = np.sum(intersection) / np.sum(union)\n",
    "    return (iou_score)\n",
    "\n",
    "# Usually there is a mask for the entire plant in addition to individual leaves. \n",
    "# This function attempts to remove the full plant mask by caclculating the iou of each mask and the union of all masks.\n",
    "def check_full_plant(masks):\n",
    "    # Initialize the combined mask\n",
    "    mask_all = np.zeros(masks[0].shape, dtype=np.float32)\n",
    "\n",
    "    # Combine all masks into one\n",
    "    for mask in masks:\n",
    "        mask_all += mask.astype(np.float32)\n",
    "\n",
    "    iou_withall = []\n",
    "    # Calculate IoU for each mask with the combined mask\n",
    "    for mask in masks:\n",
    "        iou_withall.append(iou(mask, mask_all > 0))\n",
    "\n",
    "    idx_notall = np.array(iou_withall) < 0.9\n",
    "    return idx_notall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_masks(masks, image, include, random_color=True):\n",
    "    # Convert image to RGBA\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    \n",
    "    # Iterate through each mask\n",
    "    for i in range(masks.shape[0]):\n",
    "        if (True):\n",
    "            # print(masks[i])\n",
    "            mask = masks[i]\n",
    "            if random_color:\n",
    "                color = np.concatenate([np.random.random(3), np.array([0.8])], axis=0)\n",
    "            else:\n",
    "                color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "\n",
    "            h, w = mask.shape[-2:]\n",
    "            mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "            mask_image_pil = Image.fromarray((mask_image * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "\n",
    "            # Composite the mask with the image\n",
    "            annotated_frame_pil = Image.alpha_composite(annotated_frame_pil, mask_image_pil)\n",
    "    \n",
    "    return np.array(annotated_frame_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Grounding DINO for detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_bbox_large(bbox, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Check if bbox covers a large portion of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - bbox: List representing the bounding box [x, y, width, height].\n",
    "    - threshold: Threshold percentage for considering a bbox as covering a large portion of the image.\n",
    "\n",
    "    Returns:\n",
    "    - True if bbox covers a large portion of the image, False otherwise.\n",
    "    \"\"\"\n",
    "    _, _, width, height = bbox\n",
    "    bbox_area = width * height\n",
    "    image_area = 1.0  # Assuming image area is 1 (normalized coordinates)\n",
    "    return bbox_area >= threshold * image_area\n",
    "\n",
    "def filter_large_bboxes(boxes, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Filter out bounding boxes that cover a large portion of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - boxes: Tensor of bounding boxes in the format (left, top, width, height).\n",
    "    - image_size: Tuple representing the size of the image (width, height).\n",
    "    - threshold: Threshold percentage for considering a bbox as covering a large portion of the image.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of bounding boxes that do not cover a large portion of the image.\n",
    "    \"\"\"\n",
    "    filtered_boxes = []\n",
    "    for bbox in boxes:\n",
    "        if not is_bbox_large(bbox, threshold):\n",
    "            filtered_boxes.append(bbox)\n",
    "    if len(filtered_boxes) > 0:\n",
    "        return torch.stack(filtered_boxes)\n",
    "    else:\n",
    "        # Return an empty tensor with the same shape as the input boxes\n",
    "        return torch.empty_like(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, update image_dir and output_folder to the following:\n",
    "\n",
    "+ `image_dir`: Directory where your images are\n",
    "+ `output_folder`: Directory where images visualizing segmentation results will be saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/jovyan/work/segment_anything/2024-06-04_cropped/IMG_6056.JPG\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load image\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m image_source, image \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m(file_path)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run Grounding DINO predictions\u001b[39;00m\n\u001b[1;32m     34\u001b[0m boxes, logits, phrases, scores \u001b[38;5;241m=\u001b[39m predict(\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mgroundingdino_model, \n\u001b[1;32m     36\u001b[0m     image\u001b[38;5;241m=\u001b[39mimage, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE\n\u001b[1;32m     41\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_image' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Directories\n",
    "image_dir = '/home/jovyan/work/segment_anything/2024-06-04_cropped'\n",
    "output_folder = '/home/jovyan/work/segment_anything/2025_prompt_test'\n",
    "\n",
    "# Grounding DINO settings\n",
    "TEXT_PROMPT = \"leaf or small sprouting leaf\"\n",
    "BOX_TRESHOLD = 0.3\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for file_name in os.listdir(image_dir):\n",
    "    # Check if it's an image file\n",
    "    if not file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "        continue\n",
    "    \n",
    "    name, ext = os.path.splitext(file_name)\n",
    "    file_path = os.path.join(image_dir, file_name)\n",
    "    \n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        print('File not found, skipping...')\n",
    "        continue\n",
    "\n",
    "    # Load image\n",
    "    image_source, image = load_image(file_path)\n",
    "\n",
    "    # Run Grounding DINO predictions\n",
    "    boxes, logits, phrases, scores = predict(\n",
    "        model=groundingdino_model, \n",
    "        image=image, \n",
    "        caption=TEXT_PROMPT, \n",
    "        box_threshold=BOX_TRESHOLD, \n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    H, W, _ = image_source.shape\n",
    "    boxes = filter_large_bboxes(boxes, threshold=0.9)\n",
    "    \n",
    "    if boxes.size(0) == 0:\n",
    "        print(\"No boxes detected, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Annotate image with bounding boxes\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Save Grouding DINO bbox visualization to image\n",
    "    dino_result = Image.fromarray(annotated_frame)\n",
    "    dino_result.save(os.path.join(output_folder, f\"{name}_dino_bboxes.png\"))\n",
    "    \n",
    "    # Predict masks using SAM\n",
    "    sam_predictor.set_image(image_source)\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(boxes_xyxy, image_source.shape[:2]).to(DEVICE)\n",
    "    \n",
    "    masks, _, _ = sam_predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    \n",
    "    # Process and save masks\n",
    "    masks_cpu = masks.cpu().numpy()\n",
    "    idx_notall = check_full_plant(masks_cpu)\n",
    "    \n",
    "    original_image = Image.open(file_path)\n",
    "    image_array = np.array(original_image)\n",
    "    annotated_frame_with_mask = show_masks(masks_cpu, image_array, idx_notall)\n",
    "    output_image = Image.fromarray(annotated_frame_with_mask)\n",
    "    output_image.save(os.path.join(output_folder, f\"{name}_sam_masks.png\"))\n",
    "    \n",
    "\n",
    "    print(f\"Saved results for: {file_name}\")\n",
    "\n",
    "print(\"Processing complete.\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
