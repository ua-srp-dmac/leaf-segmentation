{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da84adc-b8fa-4bef-b282-9570a810241b",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f8e53e-be11-4a80-b2c1-81f9fd6fc8bf",
   "metadata": {},
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7829e5e1-3895-438f-9bbd-82096a4fe7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install fiftyone\n",
    "!pip install pyzbar\n",
    "!pip install opencv-python\n",
    "!pip install seaborn\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07d94f-cd39-437c-81a8-91956b8da9c2",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c04cd5a-4265-41b2-aed6-6783a5fe0e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data.catalog import Metadata\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fiftyone as fo\n",
    "from PIL import Image, ImageOps\n",
    "from PIL.ExifTags import TAGS\n",
    "from pathlib import Path\n",
    "from pyzbar.pyzbar import decode\n",
    "from pyzbar.pyzbar import ZBarSymbol\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944497c1-75df-42b4-af80-4782bb070fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import transforms as T\n",
    "\n",
    "class AspectRatioResizeMapper:\n",
    "    \"\"\"Custom mapper to resize images while keeping the aspect ratio.\"\"\"\n",
    "    \n",
    "    def __init__(self, short_side_length=1000):\n",
    "        self.transform = T.ResizeShortestEdge(short_side_length, short_side_length * 2, \"range\") \n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        dataset_dict = dataset_dict.copy()  # Don't modify the original dataset\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "        \n",
    "        # Apply aspect ratio-preserving resize\n",
    "        aug_input = T.AugInput(image)\n",
    "        transforms = self.transform(aug_input)\n",
    "        image = aug_input.image  # Resized image (NumPy array)\n",
    "\n",
    "        # âœ… Convert NumPy array to a PyTorch tensor\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))  # (C, H, W) format\n",
    "\n",
    "        # Update dataset dictionary\n",
    "        dataset_dict[\"image\"] = image\n",
    "        return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67522aa8-5a62-42a3-835e-0771d36268e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import load_coco_json\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def get_leaf_only_dataset(json_path, image_dir, dataset_name):\n",
    "    \"\"\"Loads COCO dataset but keeps only the 'leaf' class annotations.\"\"\"\n",
    "\n",
    "    dataset_dicts = load_coco_json(json_path, image_dir, dataset_name)\n",
    "\n",
    "    # Dynamically get the correct index for \"leaf\"\n",
    "    metadata = MetadataCatalog.get(dataset_name)\n",
    "    leaf_class_index = metadata.thing_classes.index(\"leaf\")\n",
    "    \n",
    "    print('leaf_class_index', leaf_class_index)\n",
    "\n",
    "    filtered_dataset = []\n",
    "    for image_dict in dataset_dicts:\n",
    "        # Keep only annotations that belong to the \"leaf\" class\n",
    "        leaf_annotations = [ann for ann in image_dict[\"annotations\"] if ann[\"category_id\"] == leaf_class_index]\n",
    "\n",
    "        if leaf_annotations:  # Only keep images with at least one leaf\n",
    "            image_dict = image_dict.copy()  # Avoid modifying the original dataset\n",
    "            image_dict[\"annotations\"] = leaf_annotations\n",
    "            image_dict[\"categories\"] = [{\"id\": leaf_class_index, \"name\": \"leaf\"}]  # Update category list\n",
    "            filtered_dataset.append(image_dict)\n",
    "\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a998f5-82c3-4253-997c-a59c32e1d131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(thing_classes=['leaf', 'qr', 'red-square'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set this to today's date\n",
    "today = \"2024-03-15\"\n",
    "\n",
    "# modify output folder suffix if needed\n",
    "suffix = \"kfold_train\"\n",
    "\n",
    "# name of output folder\n",
    "output_folder_name = today + \"_\" + suffix\n",
    "\n",
    "images_folder = '/home/jovyan/work/data/2024-03-14_leaves'\n",
    "output_folder = f'/home/jovyan/work/mask_rcnn/{output_folder_name}'\n",
    "\n",
    "k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd33d2b-5c5d-4ae2-b5bf-a01167751ccc",
   "metadata": {},
   "source": [
    "### Specify Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3de97ab0-2082-446d-ba03-4c434f383997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "from tabulate import tabulate\n",
    "from detectron2.utils.logger import create_small_table\n",
    "\n",
    "class CustomCOCOEvaluator(COCOEvaluator):\n",
    "    \"\"\"Custom COCO Evaluator with modified small/medium/large thresholds.\"\"\"\n",
    "    \n",
    "    def _evaluate_predictions_on_coco(self, coco_gt, coco_results, iou_type):\n",
    "        \"\"\"Override the function to modify small/medium/large thresholds.\"\"\"\n",
    "        \n",
    "        # Call the original COCO evaluation\n",
    "        coco_eval = super()._evaluate_predictions_on_coco(coco_gt, coco_results, iou_type)\n",
    "\n",
    "        if coco_eval is None:\n",
    "            return None\n",
    "\n",
    "        # Modify thresholds (change here as needed)\n",
    "        small_threshold = 10000   # Instead of 1024\n",
    "        medium_threshold = 50000  # Instead of 9216\n",
    "\n",
    "        # Create a copy of the evaluation results to modify thresholds\n",
    "        coco_eval_copy = copy.deepcopy(coco_eval)\n",
    "\n",
    "        # Adjust area thresholds in COCO evaluation\n",
    "        coco_eval_copy.params.areaRng = [\n",
    "            [0, small_threshold],      # Small\n",
    "            [small_threshold, medium_threshold],  # Medium\n",
    "            [medium_threshold, 1e10]   # Large\n",
    "        ]\n",
    "        coco_eval_copy.params.areaRngLbl = [\"small\", \"medium\", \"large\"]\n",
    "\n",
    "        # Run evaluation again with new thresholds\n",
    "        coco_eval_copy.evaluate()\n",
    "        coco_eval_copy.accumulate()\n",
    "        coco_eval_copy.summarize()\n",
    "\n",
    "        return coco_eval_copy\n",
    "    \n",
    "    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
    "        \"\"\"\n",
    "        Derive detailed COCO evaluation metrics, including AP (0.5:0.95), AP50, AP75, APs, APm, and APl\n",
    "        per class.\n",
    "\n",
    "        Args:\n",
    "            coco_eval (COCOEval): COCO evaluation object containing results.\n",
    "            iou_type (str): Type of IoU metric (e.g., 'bbox', 'segm').\n",
    "            class_names (list[str]): List of class names.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing AP metrics.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define the standard COCO metric names\n",
    "        metrics = {\n",
    "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
    "        }[iou_type]\n",
    "\n",
    "        if coco_eval is None:\n",
    "            self._logger.warn(\"No predictions from the model!\")\n",
    "            return {metric: float(\"nan\") for metric in metrics}\n",
    "\n",
    "        # Extract the default COCO AP metrics\n",
    "        results = {\n",
    "            metric: float(coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\")\n",
    "            for idx, metric in enumerate(metrics)\n",
    "        }\n",
    "\n",
    "        self._logger.info(\n",
    "            \"Evaluation results for {}: \\n\".format(iou_type) + create_small_table(results)\n",
    "        )\n",
    "\n",
    "        if not np.isfinite(sum(results.values())):\n",
    "            self._logger.info(\"Some metrics cannot be computed and are shown as NaN.\")\n",
    "\n",
    "        if class_names is None or len(class_names) <= 1:\n",
    "            return results  # Skip per-class AP computation if no class names\n",
    "\n",
    "        # Extract per-class AP values using precision results\n",
    "        precisions = coco_eval.eval[\"precision\"]  # Shape: (IoU, Recall, Class, Area, Max Dets)\n",
    "\n",
    "        assert len(class_names) == precisions.shape[2]\n",
    "        assert coco_eval.params.iouThrs[0] == 0.5  # Ensure IoU=0.5 is first in the list\n",
    "\n",
    "        results_per_category = []\n",
    "        for idx, name in enumerate(class_names):\n",
    "            # Compute AP (mean over IoUs 0.5:0.95)\n",
    "            precision_all = precisions[:, :, idx, 0, -1]  # IoU=[0.5:0.95], all object sizes\n",
    "            precision_all = precision_all[precision_all > -1]\n",
    "            ap = np.mean(precision_all) * 100 if precision_all.size else float(\"nan\")\n",
    "\n",
    "            # Compute AP for IoU=0.5 (AP50)\n",
    "            precision_50 = precisions[0, :, idx, 0, -1]  # IoU=0.5, all object sizes\n",
    "            precision_50 = precision_50[precision_50 > -1]\n",
    "            ap50 = np.mean(precision_50) * 100 if precision_50.size else float(\"nan\")\n",
    "\n",
    "            # Compute AP for IoU=0.75 (AP75)\n",
    "            precision_75 = precisions[5, :, idx, 0, -1]  # IoU=0.75, all object sizes\n",
    "            precision_75 = precision_75[precision_75 > -1]\n",
    "            ap75 = np.mean(precision_75) * 100 if precision_75.size else float(\"nan\")\n",
    "\n",
    "            # Compute AP for small, medium, and large objects\n",
    "            precision_s = precisions[0, :, idx, 1, -1]  # Small objects\n",
    "            precision_m = precisions[0, :, idx, 2, -1]  # Medium objects\n",
    "            precision_l = precisions[0, :, idx, 3, -1]  # Large objects\n",
    "\n",
    "            ap_s = np.mean(precision_s[precision_s > -1]) * 100 if precision_s.size else float(\"nan\")\n",
    "            ap_m = np.mean(precision_m[precision_m > -1]) * 100 if precision_m.size else float(\"nan\")\n",
    "            ap_l = np.mean(precision_l[precision_l > -1]) * 100 if precision_l.size else float(\"nan\")\n",
    "\n",
    "            results_per_category.append((name, ap, ap50, ap75, ap_s, ap_m, ap_l))\n",
    "\n",
    "        # Tabulate per-category results\n",
    "        headers = [\"category\", \"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"]\n",
    "        results_flatten = list(itertools.chain(*results_per_category))\n",
    "        results_2d = itertools.zip_longest(*[results_flatten[i::7] for i in range(7)])\n",
    "        table = tabulate(results_2d, tablefmt=\"pipe\", floatfmt=\".3f\", headers=headers, numalign=\"left\")\n",
    "\n",
    "        self._logger.info(\"Per-category {} AP (AP, AP50, AP75, APs, APm, APl): \\n\".format(iou_type) + table)\n",
    "\n",
    "        # Store per-class AP in the results dictionary\n",
    "        for name, ap, ap50, ap75, ap_s, ap_m, ap_l in results_per_category:\n",
    "            results.update({\n",
    "                f\"AP-{name}\": ap,\n",
    "                f\"AP50-{name}\": ap50,\n",
    "                f\"AP75-{name}\": ap75,\n",
    "                f\"APs-{name}\": ap_s,\n",
    "                f\"APm-{name}\": ap_m,\n",
    "                f\"APl-{name}\": ap_l,\n",
    "            })\n",
    "\n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22158348-7b28-44ff-b6d9-fd39df9e4c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 02:32:29 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/fold_0/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/01 02:32:29 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[02/01 02:32:29 d2.data.datasets.coco]: \u001b[0mLoaded 36 images in COCO format from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/test_0.json\n",
      "\u001b[32m[02/01 02:32:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/01 02:32:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/01 02:32:29 d2.data.common]: \u001b[0mSerializing 36 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 02:32:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.20 MiB\n",
      "\u001b[32m[02/01 02:32:29 d2.evaluation.evaluator]: \u001b[0mStart inference on 36 batches\n",
      "\u001b[32m[02/01 02:32:53 d2.evaluation.evaluator]: \u001b[0mInference done 11/36. Dataloading: 0.0023 s/iter. Inference: 1.3647 s/iter. Eval: 0.9214 s/iter. Total: 2.2884 s/iter. ETA=0:00:57\n",
      "\u001b[32m[02/01 02:32:58 d2.evaluation.evaluator]: \u001b[0mInference done 13/36. Dataloading: 0.0025 s/iter. Inference: 1.3456 s/iter. Eval: 1.0113 s/iter. Total: 2.3599 s/iter. ETA=0:00:54\n",
      "\u001b[32m[02/01 02:33:05 d2.evaluation.evaluator]: \u001b[0mInference done 16/36. Dataloading: 0.0026 s/iter. Inference: 1.3291 s/iter. Eval: 0.9704 s/iter. Total: 2.3026 s/iter. ETA=0:00:46\n",
      "\u001b[32m[02/01 02:33:11 d2.evaluation.evaluator]: \u001b[0mInference done 19/36. Dataloading: 0.0026 s/iter. Inference: 1.3588 s/iter. Eval: 0.8701 s/iter. Total: 2.2320 s/iter. ETA=0:00:37\n",
      "\u001b[32m[02/01 02:33:18 d2.evaluation.evaluator]: \u001b[0mInference done 22/36. Dataloading: 0.0027 s/iter. Inference: 1.3947 s/iter. Eval: 0.8475 s/iter. Total: 2.2454 s/iter. ETA=0:00:31\n",
      "\u001b[32m[02/01 02:33:24 d2.evaluation.evaluator]: \u001b[0mInference done 25/36. Dataloading: 0.0027 s/iter. Inference: 1.4113 s/iter. Eval: 0.7864 s/iter. Total: 2.2010 s/iter. ETA=0:00:24\n",
      "\u001b[32m[02/01 02:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 28/36. Dataloading: 0.0028 s/iter. Inference: 1.4216 s/iter. Eval: 0.7626 s/iter. Total: 2.1875 s/iter. ETA=0:00:17\n",
      "\u001b[32m[02/01 02:33:36 d2.evaluation.evaluator]: \u001b[0mInference done 31/36. Dataloading: 0.0027 s/iter. Inference: 1.4196 s/iter. Eval: 0.7284 s/iter. Total: 2.1512 s/iter. ETA=0:00:10\n",
      "\u001b[32m[02/01 02:33:42 d2.evaluation.evaluator]: \u001b[0mInference done 34/36. Dataloading: 0.0027 s/iter. Inference: 1.4193 s/iter. Eval: 0.7128 s/iter. Total: 2.1354 s/iter. ETA=0:00:04\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.evaluator]: \u001b[0mInference done 36/36. Dataloading: 0.0027 s/iter. Inference: 1.4342 s/iter. Eval: 0.7344 s/iter. Total: 2.1718 s/iter. ETA=0:00:00\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:07.472088 (2.176519 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:44 (1.434169 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.779\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.917\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.846\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.822\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.664\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.800\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.802\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.124\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.848\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 77.921 | 91.741 | 84.578 | 0.000 | 12.388 | 82.220 |\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 48.765 | 75.224  | 53.735  | 0.000 | 32.440 | 90.733  |\n",
      "| qr         | 86.447 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 98.551 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.788\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.905\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.861\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.837\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.668\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.808\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.106\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.859\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 78.837 | 90.519 | 86.080 | 0.000 | 4.497 | 83.718 |\n",
      "\u001b[32m[02/01 02:33:47 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 50.700 | 71.556  | 58.240  | 0.000 | 10.396 | 90.443  |\n",
      "| qr         | 86.335 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 99.477 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "OrderedDict([('bbox', {'AP': 77.92105264328882, 'AP50': 91.74139525723109, 'AP75': 84.57836546846409, 'APs': 0.0, 'APm': 12.388301855395623, 'APl': 82.22028299871809, 'AP-leaf': 48.76509934321966, 'AP50-leaf': 75.22418577169326, 'AP75-leaf': 53.735096405392305, 'APs-leaf': 0.0, 'APm-leaf': 32.44030285381479, 'APl-leaf': 90.73332573429201, 'AP-qr': 86.44667952016877, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 98.55137906647808, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0}), ('segm', {'AP': 78.83711468606947, 'AP50': 90.51867247275923, 'AP75': 86.0801450322235, 'APs': 0.0, 'APm': 4.496831282507133, 'APl': 83.71821374710716, 'AP-leaf': 50.69978104228036, 'AP50-leaf': 71.55601741827768, 'AP75-leaf': 58.24043509667051, 'APs-leaf': 0.0, 'APm-leaf': 10.396039603960396, 'APl-leaf': 90.44261956478826, 'AP-qr': 86.33490106401855, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 99.47666195190948, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 02:33:48 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/fold_1/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/01 02:33:48 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[02/01 02:33:48 d2.data.datasets.coco]: \u001b[0mLoaded 35 images in COCO format from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/test_1.json\n",
      "\u001b[32m[02/01 02:33:48 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/01 02:33:48 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/01 02:33:48 d2.data.common]: \u001b[0mSerializing 35 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 02:33:48 d2.data.common]: \u001b[0mSerialized dataset takes 0.12 MiB\n",
      "\u001b[32m[02/01 02:33:48 d2.evaluation.evaluator]: \u001b[0mStart inference on 35 batches\n",
      "\u001b[32m[02/01 02:34:14 d2.evaluation.evaluator]: \u001b[0mInference done 11/35. Dataloading: 0.0020 s/iter. Inference: 1.2272 s/iter. Eval: 0.8122 s/iter. Total: 2.0414 s/iter. ETA=0:00:48\n",
      "\u001b[32m[02/01 02:34:20 d2.evaluation.evaluator]: \u001b[0mInference done 14/35. Dataloading: 0.0021 s/iter. Inference: 1.2166 s/iter. Eval: 0.7942 s/iter. Total: 2.0133 s/iter. ETA=0:00:42\n",
      "\u001b[32m[02/01 02:34:26 d2.evaluation.evaluator]: \u001b[0mInference done 17/35. Dataloading: 0.0023 s/iter. Inference: 1.2188 s/iter. Eval: 0.7985 s/iter. Total: 2.0200 s/iter. ETA=0:00:36\n",
      "\u001b[32m[02/01 02:34:31 d2.evaluation.evaluator]: \u001b[0mInference done 19/35. Dataloading: 0.0024 s/iter. Inference: 1.2217 s/iter. Eval: 0.8679 s/iter. Total: 2.0924 s/iter. ETA=0:00:33\n",
      "\u001b[32m[02/01 02:34:39 d2.evaluation.evaluator]: \u001b[0mInference done 22/35. Dataloading: 0.0025 s/iter. Inference: 1.2349 s/iter. Eval: 0.9662 s/iter. Total: 2.2040 s/iter. ETA=0:00:28\n",
      "\u001b[32m[02/01 02:34:45 d2.evaluation.evaluator]: \u001b[0mInference done 24/35. Dataloading: 0.0025 s/iter. Inference: 1.2461 s/iter. Eval: 1.0252 s/iter. Total: 2.2744 s/iter. ETA=0:00:25\n",
      "\u001b[32m[02/01 02:34:50 d2.evaluation.evaluator]: \u001b[0mInference done 26/35. Dataloading: 0.0025 s/iter. Inference: 1.2444 s/iter. Eval: 1.0514 s/iter. Total: 2.2988 s/iter. ETA=0:00:20\n",
      "\u001b[32m[02/01 02:34:57 d2.evaluation.evaluator]: \u001b[0mInference done 29/35. Dataloading: 0.0025 s/iter. Inference: 1.2413 s/iter. Eval: 1.0661 s/iter. Total: 2.3104 s/iter. ETA=0:00:13\n",
      "\u001b[32m[02/01 02:35:02 d2.evaluation.evaluator]: \u001b[0mInference done 31/35. Dataloading: 0.0025 s/iter. Inference: 1.2422 s/iter. Eval: 1.0827 s/iter. Total: 2.3280 s/iter. ETA=0:00:09\n",
      "\u001b[32m[02/01 02:35:10 d2.evaluation.evaluator]: \u001b[0mInference done 34/35. Dataloading: 0.0025 s/iter. Inference: 1.2425 s/iter. Eval: 1.0902 s/iter. Total: 2.3358 s/iter. ETA=0:00:02\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:10.402484 (2.346749 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:37 (1.244749 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.771\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.928\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.846\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.807\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.806\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.806\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.283\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.846\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 77.131 | 92.752 | 84.552 | 0.000 | 25.877 | 80.725 |\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 47.386 | 78.256  | 53.656  | 0.000 | 51.692 | 92.365  |\n",
      "| qr         | 85.324 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 98.683 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.785\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.935\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.848\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.204\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.676\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.859\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 78.520 | 93.481 | 84.791 | 0.000 | 20.445 | 82.686 |\n",
      "\u001b[32m[02/01 02:35:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 50.363 | 80.443  | 54.373  | 0.000 | 50.060 | 93.973  |\n",
      "| qr         | 85.746 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 99.450 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "OrderedDict([('bbox', {'AP': 77.13078483537672, 'AP50': 92.75214870822455, 'AP75': 84.5520120890918, 'APs': 0.0, 'APm': 25.877176696077225, 'APl': 80.72529193131544, 'AP-leaf': 47.385534679117555, 'AP50-leaf': 78.25644612467364, 'AP75-leaf': 53.65603626727542, 'APs-leaf': 0.0, 'APm-leaf': 51.69154144311295, 'APl-leaf': 92.36482765163368, 'AP-qr': 85.3242532656627, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 98.68256656134994, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0}), ('segm', {'AP': 78.5195790236678, 'AP50': 93.48113153600336, 'AP75': 84.7909632942353, 'APs': 0.0, 'APm': 20.44474491109276, 'APl': 82.68584933277118, 'AP-leaf': 50.362510406899766, 'AP50-leaf': 80.44339460801008, 'AP75-leaf': 54.372889882705934, 'APs-leaf': 0.0, 'APm-leaf': 50.060192432494055, 'APl-leaf': 93.97255997007342, 'AP-qr': 85.74590232132458, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 99.45032434277911, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 02:35:13 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/fold_2/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/01 02:35:13 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[02/01 02:35:13 d2.data.datasets.coco]: \u001b[0mLoaded 35 images in COCO format from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/test_2.json\n",
      "\u001b[32m[02/01 02:35:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/01 02:35:13 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/01 02:35:13 d2.data.common]: \u001b[0mSerializing 35 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 02:35:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.16 MiB\n",
      "\u001b[32m[02/01 02:35:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 35 batches\n",
      "\u001b[32m[02/01 02:35:35 d2.evaluation.evaluator]: \u001b[0mInference done 11/35. Dataloading: 0.0016 s/iter. Inference: 1.3103 s/iter. Eval: 0.4656 s/iter. Total: 1.7775 s/iter. ETA=0:00:42\n",
      "\u001b[32m[02/01 02:35:41 d2.evaluation.evaluator]: \u001b[0mInference done 14/35. Dataloading: 0.0019 s/iter. Inference: 1.3094 s/iter. Eval: 0.4709 s/iter. Total: 1.7825 s/iter. ETA=0:00:37\n",
      "\u001b[32m[02/01 02:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 17/35. Dataloading: 0.0020 s/iter. Inference: 1.3151 s/iter. Eval: 0.5117 s/iter. Total: 1.8293 s/iter. ETA=0:00:32\n",
      "\u001b[32m[02/01 02:35:53 d2.evaluation.evaluator]: \u001b[0mInference done 20/35. Dataloading: 0.0021 s/iter. Inference: 1.3367 s/iter. Eval: 0.5440 s/iter. Total: 1.8833 s/iter. ETA=0:00:28\n",
      "\u001b[32m[02/01 02:35:59 d2.evaluation.evaluator]: \u001b[0mInference done 23/35. Dataloading: 0.0022 s/iter. Inference: 1.3407 s/iter. Eval: 0.5790 s/iter. Total: 1.9224 s/iter. ETA=0:00:23\n",
      "\u001b[32m[02/01 02:36:05 d2.evaluation.evaluator]: \u001b[0mInference done 26/35. Dataloading: 0.0022 s/iter. Inference: 1.3481 s/iter. Eval: 0.5930 s/iter. Total: 1.9438 s/iter. ETA=0:00:17\n",
      "\u001b[32m[02/01 02:36:11 d2.evaluation.evaluator]: \u001b[0mInference done 29/35. Dataloading: 0.0022 s/iter. Inference: 1.3468 s/iter. Eval: 0.5953 s/iter. Total: 1.9448 s/iter. ETA=0:00:11\n",
      "\u001b[32m[02/01 02:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 32/35. Dataloading: 0.0022 s/iter. Inference: 1.3425 s/iter. Eval: 0.5969 s/iter. Total: 1.9421 s/iter. ETA=0:00:05\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.evaluator]: \u001b[0mInference done 35/35. Dataloading: 0.0022 s/iter. Inference: 1.3467 s/iter. Eval: 0.6213 s/iter. Total: 1.9707 s/iter. ETA=0:00:00\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:59.280009 (1.976000 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:40 (1.346654 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.781\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.927\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.861\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.258\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.667\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.807\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.809\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.848\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 78.089 | 92.743 | 86.055 | 0.000 | 25.760 | 81.766 |\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 50.572 | 78.228  | 58.166  | 0.000 | 49.168 | 91.583  |\n",
      "| qr         | 85.739 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 97.956 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.805\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.925\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.872\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.216\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.847\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.827\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.829\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.872\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 80.515 | 92.500 | 87.151 | 0.000 | 21.628 | 84.691 |\n",
      "\u001b[32m[02/01 02:36:24 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 54.438 | 77.500  | 61.452  | 0.000 | 43.890 | 90.360  |\n",
      "| qr         | 88.557 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 98.550 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "OrderedDict([('bbox', {'AP': 78.08903974322591, 'AP50': 92.74282725825825, 'AP75': 86.05519179638254, 'APs': 0.0, 'APm': 25.760259276499486, 'APl': 81.76632120744605, 'AP-leaf': 50.57172474766888, 'AP50-leaf': 78.22848177477475, 'AP75-leaf': 58.16557538914768, 'APs-leaf': 0.0, 'APm-leaf': 49.16848827739918, 'APl-leaf': 91.58269875371245, 'AP-qr': 85.73924172387589, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 97.95615275813296, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0}), ('segm', {'AP': 80.51505995114859, 'AP50': 92.49998073071338, 'AP75': 87.15050460513775, 'APs': 0.0, 'APm': 21.627564953440228, 'APl': 84.69066028656349, 'AP-leaf': 54.43761385152166, 'AP50-leaf': 77.49994219214014, 'AP75-leaf': 61.45151381541325, 'APs-leaf': 0.0, 'APm-leaf': 43.8897229549873, 'APl-leaf': 90.36023910232068, 'AP-qr': 88.55735383785058, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 98.55021216407356, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 02:36:25 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/fold_3/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/01 02:36:25 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[02/01 02:36:25 d2.data.datasets.coco]: \u001b[0mLoaded 35 images in COCO format from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/test_3.json\n",
      "\u001b[32m[02/01 02:36:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/01 02:36:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/01 02:36:25 d2.data.common]: \u001b[0mSerializing 35 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 02:36:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.15 MiB\n",
      "\u001b[32m[02/01 02:36:25 d2.evaluation.evaluator]: \u001b[0mStart inference on 35 batches\n",
      "\u001b[32m[02/01 02:36:47 d2.evaluation.evaluator]: \u001b[0mInference done 11/35. Dataloading: 0.0016 s/iter. Inference: 1.1957 s/iter. Eval: 0.6151 s/iter. Total: 1.8124 s/iter. ETA=0:00:43\n",
      "\u001b[32m[02/01 02:36:53 d2.evaluation.evaluator]: \u001b[0mInference done 14/35. Dataloading: 0.0019 s/iter. Inference: 1.1931 s/iter. Eval: 0.6436 s/iter. Total: 1.8389 s/iter. ETA=0:00:38\n",
      "\u001b[32m[02/01 02:36:59 d2.evaluation.evaluator]: \u001b[0mInference done 17/35. Dataloading: 0.0020 s/iter. Inference: 1.2021 s/iter. Eval: 0.6926 s/iter. Total: 1.8970 s/iter. ETA=0:00:34\n",
      "\u001b[32m[02/01 02:37:04 d2.evaluation.evaluator]: \u001b[0mInference done 19/35. Dataloading: 0.0020 s/iter. Inference: 1.2077 s/iter. Eval: 0.7801 s/iter. Total: 1.9902 s/iter. ETA=0:00:31\n",
      "\u001b[32m[02/01 02:37:10 d2.evaluation.evaluator]: \u001b[0mInference done 22/35. Dataloading: 0.0021 s/iter. Inference: 1.2070 s/iter. Eval: 0.7787 s/iter. Total: 1.9883 s/iter. ETA=0:00:25\n",
      "\u001b[32m[02/01 02:37:16 d2.evaluation.evaluator]: \u001b[0mInference done 25/35. Dataloading: 0.0022 s/iter. Inference: 1.2086 s/iter. Eval: 0.7666 s/iter. Total: 1.9779 s/iter. ETA=0:00:19\n",
      "\u001b[32m[02/01 02:37:21 d2.evaluation.evaluator]: \u001b[0mInference done 27/35. Dataloading: 0.0022 s/iter. Inference: 1.2086 s/iter. Eval: 0.8152 s/iter. Total: 2.0265 s/iter. ETA=0:00:16\n",
      "\u001b[32m[02/01 02:37:27 d2.evaluation.evaluator]: \u001b[0mInference done 29/35. Dataloading: 0.0022 s/iter. Inference: 1.2174 s/iter. Eval: 0.8817 s/iter. Total: 2.1018 s/iter. ETA=0:00:12\n",
      "\u001b[32m[02/01 02:37:33 d2.evaluation.evaluator]: \u001b[0mInference done 32/35. Dataloading: 0.0022 s/iter. Inference: 1.2233 s/iter. Eval: 0.8989 s/iter. Total: 2.1250 s/iter. ETA=0:00:06\n",
      "\u001b[32m[02/01 02:37:39 d2.evaluation.evaluator]: \u001b[0mInference done 35/35. Dataloading: 0.0022 s/iter. Inference: 1.2174 s/iter. Eval: 0.8886 s/iter. Total: 2.1088 s/iter. ETA=0:00:00\n",
      "\u001b[32m[02/01 02:37:39 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:03.426136 (2.114205 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:37:39 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:36 (1.217389 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.769\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.905\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.838\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.120\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.810\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.665\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.797\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.129\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.847\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 76.928 | 90.541 | 83.804 | 0.000 | 12.034 | 80.958 |\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 46.139 | 71.623  | 51.412  | 0.000 | 23.269 | 88.659  |\n",
      "| qr         | 86.118 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 98.526 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.766\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.907\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.842\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.664\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.797\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.848\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 76.599 | 90.725 | 84.204 | 0.000 | 9.017 | 80.922 |\n",
      "\u001b[32m[02/01 02:37:40 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 47.301 | 72.176  | 52.612  | 0.000 | 16.390 | 90.478  |\n",
      "| qr         | 83.178 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 99.318 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "OrderedDict([('bbox', {'AP': 76.92763632222854, 'AP50': 90.5410454874389, 'AP75': 83.80407350430697, 'APs': 0.0, 'APm': 12.034427489290621, 'APl': 80.95812971031593, 'AP-leaf': 46.13900639908895, 'AP50-leaf': 71.62313646231671, 'AP75-leaf': 51.412220512920904, 'APs-leaf': 0.0, 'APm-leaf': 23.26875544697327, 'APl-leaf': 88.65912728559078, 'AP-qr': 86.11804998233814, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 98.52585258525852, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0}), ('segm', {'AP': 76.59885724384766, 'AP50': 90.72518794695961, 'AP75': 84.20411685810083, 'APs': 0.0, 'APm': 9.016821226824227, 'APl': 80.92196076547928, 'AP-leaf': 47.30112933015997, 'AP50-leaf': 72.17556384087884, 'AP75-leaf': 52.6123505743025, 'APs-leaf': 0.0, 'APm-leaf': 16.390423889121127, 'APl-leaf': 90.47784492744564, 'AP-qr': 83.17751060820368, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 99.31793179317931, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0})])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 02:37:40 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/fold_4/model_final.pth ...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[02/01 02:37:41 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[02/01 02:37:41 d2.data.datasets.coco]: \u001b[0mLoaded 35 images in COCO format from /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/test_4.json\n",
      "\u001b[32m[02/01 02:37:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[02/01 02:37:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[02/01 02:37:41 d2.data.common]: \u001b[0mSerializing 35 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 02:37:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.16 MiB\n",
      "\u001b[32m[02/01 02:37:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 35 batches\n",
      "\u001b[32m[02/01 02:38:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/35. Dataloading: 0.0015 s/iter. Inference: 1.3806 s/iter. Eval: 0.4671 s/iter. Total: 1.8491 s/iter. ETA=0:00:44\n",
      "\u001b[32m[02/01 02:38:09 d2.evaluation.evaluator]: \u001b[0mInference done 14/35. Dataloading: 0.0018 s/iter. Inference: 1.3829 s/iter. Eval: 0.4519 s/iter. Total: 1.8370 s/iter. ETA=0:00:38\n",
      "\u001b[32m[02/01 02:38:14 d2.evaluation.evaluator]: \u001b[0mInference done 17/35. Dataloading: 0.0020 s/iter. Inference: 1.3777 s/iter. Eval: 0.4474 s/iter. Total: 1.8275 s/iter. ETA=0:00:32\n",
      "\u001b[32m[02/01 02:38:20 d2.evaluation.evaluator]: \u001b[0mInference done 20/35. Dataloading: 0.0020 s/iter. Inference: 1.3875 s/iter. Eval: 0.4973 s/iter. Total: 1.8873 s/iter. ETA=0:00:28\n",
      "\u001b[32m[02/01 02:38:26 d2.evaluation.evaluator]: \u001b[0mInference done 23/35. Dataloading: 0.0021 s/iter. Inference: 1.3799 s/iter. Eval: 0.5217 s/iter. Total: 1.9042 s/iter. ETA=0:00:22\n",
      "\u001b[32m[02/01 02:38:32 d2.evaluation.evaluator]: \u001b[0mInference done 26/35. Dataloading: 0.0022 s/iter. Inference: 1.3843 s/iter. Eval: 0.5242 s/iter. Total: 1.9112 s/iter. ETA=0:00:17\n",
      "\u001b[32m[02/01 02:38:38 d2.evaluation.evaluator]: \u001b[0mInference done 29/35. Dataloading: 0.0023 s/iter. Inference: 1.3757 s/iter. Eval: 0.5239 s/iter. Total: 1.9023 s/iter. ETA=0:00:11\n",
      "\u001b[32m[02/01 02:38:44 d2.evaluation.evaluator]: \u001b[0mInference done 32/35. Dataloading: 0.0023 s/iter. Inference: 1.3755 s/iter. Eval: 0.5631 s/iter. Total: 1.9413 s/iter. ETA=0:00:05\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.evaluator]: \u001b[0mInference done 35/35. Dataloading: 0.0023 s/iter. Inference: 1.3776 s/iter. Eval: 0.5945 s/iter. Total: 1.9748 s/iter. ETA=0:00:00\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:59.384395 (1.979480 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:41 (1.377621 s / iter per device, on 1 devices)\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output/coco_instances_results.json\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.01 seconds.\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.752\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.896\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.812\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.176\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.785\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.667\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.785\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.819\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 75.241 | 89.625 | 81.209 | 0.000 | 17.597 | 78.467 |\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 41.644 | 68.875  | 43.628  | 0.000 | 37.612 | 81.807  |\n",
      "| qr         | 86.517 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 97.561 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[02/01 02:38:51 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[02/01 02:38:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[02/01 02:38:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[02/01 02:38:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.760\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.896\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.825\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.131\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.795\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.672\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.784\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.791\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.827\n",
      "\u001b[32m[02/01 02:38:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
      "| 75.957 | 89.565 | 82.493 | 0.000 | 13.102 | 79.483 |\n",
      "\u001b[32m[02/01 02:38:52 d2.evaluation.coco_evaluation]: \u001b[0mPer-category segm AP (AP, AP50, AP75, APs, APm, APl): \n",
      "| category   | AP     | AP50    | AP75    | APs   | APm    | APl     |\n",
      "|:-----------|:-------|:--------|:--------|:------|:-------|:--------|\n",
      "| leaf       | 42.205 | 68.695  | 47.480  | 0.000 | 27.906 | 83.340  |\n",
      "| qr         | 86.369 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "| red-square | 99.297 | 100.000 | 100.000 | nan   | nan    | 100.000 |\n",
      "OrderedDict([('bbox', {'AP': 75.24050242835698, 'AP50': 89.62505031901252, 'AP75': 81.209355515432, 'APs': 0.0, 'APm': 17.597452912800847, 'APl': 78.4673623996079, 'AP-leaf': 41.64418644953911, 'AP50-leaf': 68.87515095703758, 'AP75-leaf': 43.62806654629598, 'APs-leaf': 0.0, 'APm-leaf': 37.61233815689262, 'APl-leaf': 81.80722838082485, 'AP-qr': 86.5166516651665, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 97.56066917036532, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0}), ('segm', {'AP': 75.95669951154137, 'AP50': 89.56501441799948, 'AP75': 82.49323918737855, 'APs': 0.0, 'APm': 13.102396629441056, 'APl': 79.48326205658101, 'AP-leaf': 42.204801955705165, 'AP50-leaf': 68.69504325399843, 'AP75-leaf': 47.479717562135626, 'APs-leaf': 0.0, 'APm-leaf': 27.90639387825422, 'APl-leaf': 83.33981911917367, 'AP-qr': 86.36860828940036, 'AP50-qr': 100.0, 'AP75-qr': 100.0, 'APs-qr': nan, 'APm-qr': nan, 'APl-qr': 100.0, 'AP-red-square': 99.29668828951861, 'AP50-red-square': 100.0, 'AP75-red-square': 100.0, 'APs-red-square': nan, 'APm-red-square': nan, 'APl-red-square': 100.0})])\n",
      "\n",
      "===== Average Segmentation AP Across 5 Folds =====\n",
      "Avg AP@0.5:0.95: 49.00\n",
      "Avg AP@0.5: 74.07\n",
      "Avg AP@0.75: 54.83\n",
      "Avg AP (Small): 0.00\n",
      "Avg AP (Medium): 29.73\n",
      "Avg AP (Large): 89.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data.catalog import Metadata\n",
    "\n",
    "\n",
    "# Assuming leaf_predictor is defined elsewhere and dataset is an iterable of samples\n",
    "datasets = {}\n",
    "\n",
    "def maybe_unregister(name):\n",
    "    if name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(name)\n",
    "    if name in MetadataCatalog.list():\n",
    "        MetadataCatalog.remove(name)\n",
    "    \n",
    "        \n",
    "segm_ap_list = []\n",
    "segm_ap_50_list = []\n",
    "segm_ap_75_list = []\n",
    "segm_ap_small_list = []\n",
    "segm_ap_medium_list = []\n",
    "segm_ap_large_list = []\n",
    "\n",
    "\n",
    "for fold in range(0, 5):\n",
    "    \n",
    "    # path to model to evaluate\n",
    "    model_path = f\"{output_folder}/fold_{fold}/model_final.pth\"\n",
    "    dataset_name = f\"test_{fold}\"\n",
    "    coco_json_path = f\"{output_folder}/test_{fold}.json\"\n",
    "    maybe_unregister(dataset_name)\n",
    "\n",
    "    register_coco_instances(dataset_name, {}, coco_json_path, images_folder)\n",
    "    \n",
    "    \n",
    "    leaf_cfg = get_cfg()\n",
    "    leaf_cfg.MODEL.DEVICE='cpu'\n",
    "    leaf_cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    leaf_cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
    "    leaf_cfg.MODEL.WEIGHTS = model_path # path to trained weights\n",
    "    leaf_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set a custom testing threshold\n",
    "\n",
    "    leaf_predictor = DefaultPredictor(leaf_cfg)\n",
    "    val_loader = build_detection_test_loader(leaf_cfg, dataset_name)\n",
    "    \n",
    "    evaluator = CustomCOCOEvaluator(dataset_name, (\"segm\",\"bbox\"), False, output_dir=\"./output\")\n",
    "    results = inference_on_dataset(leaf_predictor.model, val_loader, evaluator)\n",
    "    print(results)\n",
    "    \n",
    "    segm_results = results[\"segm\"]\n",
    "    \n",
    "    # Extract segmentation AP for \"leaf\"\n",
    "    segm_ap_list.append(results[\"segm\"][\"AP-leaf\"])\n",
    "    segm_ap_50_list.append(results[\"segm\"][\"AP50-leaf\"])\n",
    "    segm_ap_75_list.append(results[\"segm\"][\"AP75-leaf\"])\n",
    "    segm_ap_small_list.append(results[\"segm\"][\"APs-leaf\"])\n",
    "    segm_ap_medium_list.append(results[\"segm\"][\"APm-leaf\"])\n",
    "    segm_ap_large_list.append(results[\"segm\"][\"APl-leaf\"])\n",
    "\n",
    "avg_segm_ap = np.mean(segm_ap_list)\n",
    "avg_segm_ap_50 = np.mean(segm_ap_50_list)\n",
    "avg_segm_ap_75 = np.mean(segm_ap_75_list)\n",
    "avg_segm_ap_small = np.mean(segm_ap_small_list)\n",
    "avg_segm_ap_medium = np.mean(segm_ap_medium_list)\n",
    "avg_segm_ap_large = np.mean(segm_ap_large_list)\n",
    "\n",
    "print(\"\\n===== Average Segmentation AP Across 5 Folds =====\")\n",
    "print(f\"Avg AP@0.5:0.95: {avg_segm_ap:.2f}\")\n",
    "print(f\"Avg AP@0.5: {avg_segm_ap_50:.2f}\")\n",
    "print(f\"Avg AP@0.75: {avg_segm_ap_75:.2f}\")\n",
    "print(f\"Avg AP (Small): {avg_segm_ap_small:.2f}\")\n",
    "print(f\"Avg AP (Medium): {avg_segm_ap_medium:.2f}\")\n",
    "print(f\"Avg AP (Large): {avg_segm_ap_large:.2f}\")\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957cb80-2215-43b1-b4a5-bf2022ff5c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0048e453-a4ee-4055-89ff-cf49b8ab4458",
   "metadata": {},
   "source": [
    "There are several output folders from training multiple models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437332e-f435-4de1-a050-0676852694d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e1c2c63-d409-461f-b06a-4b8afbd404ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Average Segmentation AP for Leaf Across 5 Folds =====\n",
      "      Fold  Segm AP@0.5:0.95  Segm AP@0.5  Segm AP@0.75  Segm AP (Small)  \\\n",
      "0        1         50.699781    71.556017     58.240435              0.0   \n",
      "1        2         50.362510    80.443395     54.372890              0.0   \n",
      "2        3         54.437614    77.499942     61.451514              0.0   \n",
      "3        4         47.301129    72.175564     52.612351              0.0   \n",
      "4        5         42.204802    68.695043     47.479718              0.0   \n",
      "5  Average         49.001167    74.073992     54.831381              0.0   \n",
      "\n",
      "   Segm AP (Medium)  Segm AP (Large)  \n",
      "0         10.396040        90.442620  \n",
      "1         50.060192        93.972560  \n",
      "2         43.889723        90.360239  \n",
      "3         16.390424        90.477845  \n",
      "4         27.906394        83.339819  \n",
      "5         29.728555        89.718617  \n",
      "\n",
      "Results saved to /home/jovyan/work/mask_rcnn/2024-03-15_kfold_train/leaf_mAP_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Fold\": list(range(1, 6)) + [\"Average\"],\n",
    "    \"Segm AP@0.5:0.95\": segm_ap_list + [avg_segm_ap],\n",
    "    \"Segm AP@0.5\": segm_ap_50_list + [avg_segm_ap_50],\n",
    "    \"Segm AP@0.75\": segm_ap_75_list + [avg_segm_ap_75],\n",
    "    \"Segm AP (Small)\": segm_ap_small_list + [avg_segm_ap_small],\n",
    "    \"Segm AP (Medium)\": segm_ap_medium_list + [avg_segm_ap_medium],\n",
    "    \"Segm AP (Large)\": segm_ap_large_list + [avg_segm_ap_large],\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = f\"{output_folder}/leaf_mAP_results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\n===== Average Segmentation AP for Leaf Across 5 Folds =====\")\n",
    "print(df)\n",
    "print(f\"\\nResults saved to {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee89ac-4200-4083-b5ba-ec8cec744fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
